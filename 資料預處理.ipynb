{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Jiazhi\\Kaggle Repo\\CRSum+SF\\helper\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jiazhi\\AppData\\Local\\Temp\\jieba.ue68fbcada6799c78c240a9f241fbd839.cache\n",
      "Loading model cost 1.119 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "import jieba\n",
    "jieba.set_dictionary('helper/dict.txt.big')\n",
    "jieba.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取文本、打亂、並刪除null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (39990, 4)\n"
     ]
    }
   ],
   "source": [
    "corpus_df = pd.read_pickle(\"data/corpus.pkl\").dropna().sample(frac=1.0)\n",
    "print(\"Shape:\", corpus_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 對所有文本做分句分詞\n",
    "### 檢查文章開頭是否為特別標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_words = [\"中國時報\", \"工商時報\", \"旺報\", \"▲\"]\n",
    "def contains_header_words(string):\n",
    "    contain = False\n",
    "    for word in header_words:\n",
    "        contain = contain or word in string\n",
    "    return contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_sentences(document):\n",
    "    try:\n",
    "        buffer, sentences = \"\", []\n",
    "        header_tag, skip_flag = False, False\n",
    "\n",
    "        for i, char in enumerate(document):\n",
    "            if skip_flag:\n",
    "                skip_flag = False\n",
    "                continue\n",
    "\n",
    "            buffer += ' ' if char == '\\n' else char\n",
    "\n",
    "            # Check if head of corpus is special tag\n",
    "            if not sentences:\n",
    "                if char in \"(（【\":\n",
    "                    if not buffer[:-1].strip() or contains_header_words(buffer):\n",
    "                        header_tag = True\n",
    "                        continue\n",
    "\n",
    "                if not header_tag and char == '導' and '／' in buffer and '報導' in buffer:\n",
    "                    sentence = buffer.strip()\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        buffer = \"\"\n",
    "\n",
    "            if header_tag:\n",
    "                if char in \"】）)\":\n",
    "                    sentence = buffer.strip()\n",
    "                    sentences.append(sentence)\n",
    "                    buffer, header_tag = \"\", False\n",
    "                    continue\n",
    "\n",
    "            # Punc-based sentence seperation      \n",
    "            if char in '，。?？!！:：;；':\n",
    "                try:\n",
    "                    if document[i+1] == \"」\":\n",
    "                        buffer += document[i+1]\n",
    "                        skip_flag = True\n",
    "                except: None\n",
    "\n",
    "                sentence = buffer.strip()\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    buffer = \"\"\n",
    "\n",
    "        sentence = buffer.strip()\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "        \n",
    "        return sentences if sentences else None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句並刪除null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fe1fecfdfc4b5798d1deeb0a3980c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='分句', max=39990), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (39863, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21823</th>\n",
       "      <td>21823</td>\n",
       "      <td>中央社</td>\n",
       "      <td>首輛菸害體驗車 用遊戲扎根菸害防制</td>\n",
       "      <td>[（中央社記者許秩維台北4日電）, 為了讓學生瞭解菸害造成的影響和傷害，, 教育部與衛生福利...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27391</th>\n",
       "      <td>27391</td>\n",
       "      <td>聯合新聞網_即時新聞</td>\n",
       "      <td>7旬翁痛失愛車 女警細心辦案及時尋回</td>\n",
       "      <td>[嘉義縣7旬吳姓民眾年歲已大，, 行動較不靈活，, 無法步行離家太遠，, 平時就喜歡騎乘心愛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24605</th>\n",
       "      <td>24605</td>\n",
       "      <td>蘋果日報</td>\n",
       "      <td>葉文淇改名求轉運 變身26號葉家淇</td>\n",
       "      <td>[桃猿左投葉文淇為求轉運，, 去年球季後段改名葉家淇，, 今年背號也由75號改成26號。, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18152</th>\n",
       "      <td>18152</td>\n",
       "      <td>三立新聞</td>\n",
       "      <td>逃不過警犬鼻子…槍擊犯拒捕　掐牠脖子狠咬頭部還是輸了</td>\n",
       "      <td>[美國新罕布夏州有一名槍擊犯拒捕，, 藏匿在一間民宅，, 警方找來警犬吠陀（Veda）搜捕，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26482</th>\n",
       "      <td>26482</td>\n",
       "      <td>聯合新聞網_即時新聞</td>\n",
       "      <td>MIT地板清潔神器登場 吸擦噴拖樣樣在行</td>\n",
       "      <td>[年前掃除話題夯，, 想要輕鬆搞定地板清潔，, 現在又有款台灣研發製造的新商品強勢登場！, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      source                     summary  \\\n",
       "21823  21823         中央社           首輛菸害體驗車 用遊戲扎根菸害防制   \n",
       "27391  27391  聯合新聞網_即時新聞          7旬翁痛失愛車 女警細心辦案及時尋回   \n",
       "24605  24605        蘋果日報           葉文淇改名求轉運 變身26號葉家淇   \n",
       "18152  18152        三立新聞  逃不過警犬鼻子…槍擊犯拒捕　掐牠脖子狠咬頭部還是輸了   \n",
       "26482  26482  聯合新聞網_即時新聞        MIT地板清潔神器登場 吸擦噴拖樣樣在行   \n",
       "\n",
       "                                                document  \n",
       "21823  [（中央社記者許秩維台北4日電）, 為了讓學生瞭解菸害造成的影響和傷害，, 教育部與衛生福利...  \n",
       "27391  [嘉義縣7旬吳姓民眾年歲已大，, 行動較不靈活，, 無法步行離家太遠，, 平時就喜歡騎乘心愛...  \n",
       "24605  [桃猿左投葉文淇為求轉運，, 去年球季後段改名葉家淇，, 今年背號也由75號改成26號。, ...  \n",
       "18152  [美國新罕布夏州有一名槍擊犯拒捕，, 藏匿在一間民宅，, 警方找來警犬吠陀（Veda）搜捕，...  \n",
       "26482  [年前掃除話題夯，, 想要輕鬆搞定地板清潔，, 現在又有款台灣研發製造的新商品強勢登場！, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"分句\")\n",
    "corpus_df.document = corpus_df.document.progress_apply(parse_to_sentences).dropna()\n",
    "corpus_df = corpus_df.dropna()\n",
    "\n",
    "print(\"Shape:\", corpus_df.shape)\n",
    "corpus_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對內文與標題作分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205cd490a2404253addbc0d709eb7f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='內文分詞', max=39863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ded44130d4fe897668a2c4e4070d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='標題分詞', max=39863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm_notebook.pandas(desc=\"內文分詞\")\n",
    "corpus_df.document = corpus_df.document.progress_apply(lambda x: [list(jieba.cut(sen)) for sen in x])\n",
    "tqdm_notebook.pandas(desc=\"標題分詞\")\n",
    "corpus_df.summary = corpus_df.summary.progress_apply(lambda x: list(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除標題長度不足2之數據（會無法計算bigram）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_invalid_summary(summary):\n",
    "    try:\n",
    "        return summary if len(summary) >= 2 else None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee277ebfdc4472a91d4c785d1efee0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='標題驗證', max=39863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (39833, 4)\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook.pandas(desc=\"標題驗證\")\n",
    "corpus_df.summary = corpus_df.summary.progress_apply(delete_invalid_summary)\n",
    "corpus_df = corpus_df.dropna()\n",
    "print(\"Shape:\", corpus_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 儲存id, summary備用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df[['id', 'summary']].to_pickle(\"data/summary.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取各20x(10%)筆資料作為驗證集與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (31873, 4)\n",
      "Valid shape: (3980, 4)\n",
      "Test shape:  (3980, 4)\n"
     ]
    }
   ],
   "source": [
    "one_tenth = len(corpus_df) // 200\n",
    "\n",
    "train_df, valid_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "for source in corpus_df.source.unique():\n",
    "    test_df = pd.concat([test_df, corpus_df.loc[corpus_df.source==source][:one_tenth]])\n",
    "    valid_df = pd.concat([valid_df, corpus_df.loc[corpus_df.source==source][one_tenth:one_tenth*2]])\n",
    "    train_df = pd.concat([train_df, corpus_df.loc[corpus_df.source==source][one_tenth*2:]])\n",
    "    \n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Valid shape:\", valid_df.shape)\n",
    "print(\"Test shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立每篇文章的tf table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id_word2tf = {}\n",
    "for _, row in corpus_df.iterrows():\n",
    "    flatten_doc = [word for sen in row.document for word in sen]\n",
    "    \n",
    "    doc_len = len(flatten_doc)\n",
    "    counter = Counter(flatten_doc)\n",
    "    for word in counter:\n",
    "        counter[word] /= doc_len\n",
    "        \n",
    "    id_word2tf[row.id] = counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立並儲存df table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大df的詞：'，' - 0.9801\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2df = defaultdict(lambda: 0)\n",
    "for _, row in train_df.iterrows():\n",
    "    for word in set([word for sen in row.document for word in sen]):\n",
    "        word2df[word] += 1\n",
    "        \n",
    "doc_count = len(train_df)\n",
    "for word in word2df:\n",
    "    word2df[word] /= doc_count\n",
    "\n",
    "max_df_word = max(word2df.keys(), key=(lambda key: word2df[key]))\n",
    "print(\"最大df的詞：'%s' - %.4f\" % (max_df_word, word2df[max_df_word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"helper/word2df.dill\", 'wb') as file:\n",
    "    dill.dump(word2df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把文章堆砌成本文與前後文、padding、計算pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 5, 5\n",
    "\n",
    "def get_sen_labels():\n",
    "    sen_labels = []\n",
    "    for i in reversed(range(M)):\n",
    "        sen_labels.append('stm'+str(i+1))\n",
    "    sen_labels.append('st')\n",
    "    for i in range(N):\n",
    "        sen_labels.append('stn'+str(i+1))\n",
    "    return sen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentence):\n",
    "    sentence.insert(0, '<L>')\n",
    "    sentence.append('<R>')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id_list, sen_list, sentences_list, pos_list = [], [], [], []\n",
    "for _, row in train_df.iterrows():\n",
    "    id, document = row.id, row.document\n",
    "    doc_len = len([word for sen in document for word in sen])\n",
    "    \n",
    "    word_cursor = 0\n",
    "    for sentence in document:\n",
    "        word_cursor += len(sentence)\n",
    "        pos_list.append(word_cursor / doc_len)\n",
    "    \n",
    "    document = list(map(padding, document))\n",
    "    for _ in range(M):\n",
    "        document.insert(0, [])\n",
    "    for _ in range(N):\n",
    "        document.append([])\n",
    "    \n",
    "    for i in range(len(document)-M-N):\n",
    "        id_list.append(row.id)\n",
    "        sen_list.append(i)\n",
    "        sentences_list.append(document[i:i+M+N+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.DataFrame(id_list)\n",
    "sen_df = pd.DataFrame(sen_list)\n",
    "sentences_df = pd.DataFrame(sentences_list)\n",
    "pos_df = pd.DataFrame(pos_list)\n",
    "\n",
    "sentence_df = pd.concat([id_df, sen_df, sentences_df], axis=1)\n",
    "sentence_df.columns = ['id', 'sen'] + get_sen_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算其他surface features與ROUGE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(row):\n",
    "    tf, word2tf = 0, id_word2tf[row.id]\n",
    "    for word in row.st:\n",
    "        tf += word2tf[word]\n",
    "    return tf / len(row.st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(row):\n",
    "    df = 0\n",
    "    for word in row.st:\n",
    "        df += word2df[word]\n",
    "    return df / len(row.st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(sentence):\n",
    "    bigram = set()\n",
    "    for i in range(0, len(sentence)-1):\n",
    "        bigram.add((sentence[i], sentence[i+1]))\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiazhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\generic.py:4401: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "summary_bigrams_df = corpus_df[['id', 'summary']]\n",
    "summary_bigrams_df.summary = summary_bigrams_df.summary.apply(get_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_2(row):\n",
    "    if len(row.st) < 2:\n",
    "        return 0\n",
    "    \n",
    "    st_bigrams = get_bigrams(row.st)\n",
    "    summary_bigrams = summary_bigrams_df.loc[summary_bigrams_df.id == row.id].summary.values[0]\n",
    "    \n",
    "    overlap = 0\n",
    "    for bigram in st_bigrams:\n",
    "        overlap += bigram in summary_bigrams\n",
    "    return overlap / len(summary_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01539e1d3d44dae940212a3a4b03e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='len', max=1249873), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c12398674d44339a20e4c6bab2d800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='tf', max=1249873), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a4d1c82ba647ef87dbd96c90a50e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='df', max=1249873), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6668e255af8436e90fc468471a661da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ROUGE-2', max=1249873), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 13min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm_notebook.pandas(desc=\"len\")\n",
    "len_df = sentence_df.st.progress_apply(lambda x: len(x)-2)\n",
    "tqdm_notebook.pandas(desc=\"tf\")\n",
    "tf_df = sentence_df.progress_apply(get_tf, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"df\")\n",
    "df_df = sentence_df.progress_apply(get_df, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"ROUGE-2\")\n",
    "rouge_2_df = sentence_df.progress_apply(get_rouge_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1249873, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sen</th>\n",
       "      <th>stm2</th>\n",
       "      <th>stm1</th>\n",
       "      <th>st</th>\n",
       "      <th>stn1</th>\n",
       "      <th>stn2</th>\n",
       "      <th>len</th>\n",
       "      <th>pos</th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "      <th>rouge_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6939</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&lt;L&gt;, (, 新增, ：, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 動, 新聞, ), &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, &lt;R&gt;]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6939</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[&lt;L&gt;, (, 新增, ：, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 動, 新聞, ), &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, &lt;R&gt;]</td>\n",
       "      <td>3</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.112396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6939</td>\n",
       "      <td>2</td>\n",
       "      <td>[&lt;L&gt;, (, 新增, ：, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 動, 新聞, ), &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, &lt;R&gt;]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.146139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6939</td>\n",
       "      <td>3</td>\n",
       "      <td>[&lt;L&gt;, 動, 新聞, ), &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 獨自, 在, 台北市, 市民, 大道, 、, 中山北路, 口, 附近, 遛狗, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.058524</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.180605</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6939</td>\n",
       "      <td>4</td>\n",
       "      <td>[&lt;L&gt;, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, &lt;R&gt;]</td>\n",
       "      <td>[&lt;L&gt;, 獨自, 在, 台北市, 市民, 大道, 、, 中山北路, 口, 附近, 遛狗, ...</td>\n",
       "      <td>[&lt;L&gt;, 突遭, 一名, 壯漢, 尾隨, 、, 持刀, 抵住, 脖子, 搶劫, ，, &lt;R&gt;]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.081425</td>\n",
       "      <td>0.014573</td>\n",
       "      <td>0.159380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  sen                                   stm2  \\\n",
       "0  6939    0                                     []   \n",
       "1  6939    1                                     []   \n",
       "2  6939    2                   [<L>, (, 新增, ：, <R>]   \n",
       "3  6939    3                   [<L>, 動, 新聞, ), <R>]   \n",
       "4  6939    4  [<L>, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, <R>]   \n",
       "\n",
       "                                                stm1  \\\n",
       "0                                                 []   \n",
       "1                               [<L>, (, 新增, ：, <R>]   \n",
       "2                               [<L>, 動, 新聞, ), <R>]   \n",
       "3              [<L>, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, <R>]   \n",
       "4  [<L>, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, <R>]   \n",
       "\n",
       "                                                  st  \\\n",
       "0                               [<L>, (, 新增, ：, <R>]   \n",
       "1                               [<L>, 動, 新聞, ), <R>]   \n",
       "2              [<L>, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, <R>]   \n",
       "3  [<L>, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, <R>]   \n",
       "4        [<L>, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, <R>]   \n",
       "\n",
       "                                                stn1  \\\n",
       "0                               [<L>, 動, 新聞, ), <R>]   \n",
       "1              [<L>, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, <R>]   \n",
       "2  [<L>, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, <R>]   \n",
       "3        [<L>, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, <R>]   \n",
       "4  [<L>, 獨自, 在, 台北市, 市民, 大道, 、, 中山北路, 口, 附近, 遛狗, ...   \n",
       "\n",
       "                                                stn2  len       pos        tf  \\\n",
       "0              [<L>, 台北市, 中心, 又, 見, 隨機性, 侵狼, ，, <R>]    3  0.007634  0.002545   \n",
       "1  [<L>, 這次, 嫌犯, 竟是, 曾, 在, 海軍陸戰隊, 服役, 的, 壯漢, ！, <R>]    3  0.015267  0.002036   \n",
       "2        [<L>, 一名, 年輕, 女子, 數天, 前, 凌晨, 2, 點多, ，, <R>]    7  0.033079  0.015267   \n",
       "3  [<L>, 獨自, 在, 台北市, 市民, 大道, 、, 中山北路, 口, 附近, 遛狗, ...   10  0.058524  0.006573   \n",
       "4   [<L>, 突遭, 一名, 壯漢, 尾隨, 、, 持刀, 抵住, 脖子, 搶劫, ，, <R>]    9  0.081425  0.014573   \n",
       "\n",
       "         df  rouge_2  \n",
       "0  0.146939      0.0  \n",
       "1  0.112396      0.0  \n",
       "2  0.146139      0.0  \n",
       "3  0.180605      0.0  \n",
       "4  0.159380      0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([sentence_df, len_df, pos_df, tf_df, df_df, rouge_2_df], axis=1)\n",
    "train_df.columns = ['id', 'sen'] + get_sen_labels() + ['len', 'pos', 'tf', 'df', 'rouge_2']\n",
    "\n",
    "print(\"Shape:\", train_df.shape)\n",
    "train_df[['id', 'sen', 'stm2', 'stm1', 'st', 'stn1', 'stn2', 'len', 'pos', 'tf', 'df', 'rouge_2']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiazhi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "len_scaler = MinMaxScaler()\n",
    "train_df.len = len_scaler.fit_transform(train_df.len.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"helper/len-scaler.pkl\", 'wb') as file:\n",
    "    pickle.dump(len_scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('data/train_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 驗證集\n",
    "### df table與scaler拿訓練集的來用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list, sen_list, sentences_list, pos_list = [], [], [], []\n",
    "for _, row in valid_df.iterrows():\n",
    "    id, document = row.id, row.document\n",
    "    doc_len = len([word for sen in document for word in sen])\n",
    "    \n",
    "    word_cursor = 0\n",
    "    for sentence in document:\n",
    "        word_cursor += len(sentence)\n",
    "        pos_list.append(word_cursor / doc_len)\n",
    "    \n",
    "    document = list(map(padding, document))\n",
    "    for _ in range(M):\n",
    "        document.insert(0, [])\n",
    "    for _ in range(N):\n",
    "        document.append([])\n",
    "    \n",
    "    for i in range(len(document)-M-N):\n",
    "        id_list.append(row.id)\n",
    "        sen_list.append(i)\n",
    "        sentences_list.append(document[i:i+M+N+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.DataFrame(id_list)\n",
    "sen_df = pd.DataFrame(sen_list)\n",
    "sentences_df = pd.DataFrame(sentences_list)\n",
    "pos_df = pd.DataFrame(pos_list)\n",
    "\n",
    "sentence_df = pd.concat([id_df, sen_df, sentences_df], axis=1)\n",
    "sentence_df.columns = ['id', 'sen'] + get_sen_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6bac7842684bbe8366cbe4dd919e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='len', max=155691), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b9fd56261a4d919c542336644ab425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='tf', max=155691), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b7013157924a43857b5c8d188dac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='df', max=155691), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf9dd63d0dc47c08eb8295443562fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ROUGE-2', max=155691), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm_notebook.pandas(desc=\"len\")\n",
    "len_df = sentence_df.st.progress_apply(lambda x: len(x)-2)\n",
    "tqdm_notebook.pandas(desc=\"tf\")\n",
    "tf_df = sentence_df.progress_apply(get_tf, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"df\")\n",
    "df_df = sentence_df.progress_apply(get_df, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"ROUGE-2\")\n",
    "rouge_2_df = sentence_df.progress_apply(get_rouge_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (155691, 18)\n"
     ]
    }
   ],
   "source": [
    "valid_df = pd.concat([sentence_df, len_df, pos_df, tf_df, df_df, rouge_2_df], axis=1)\n",
    "valid_df.columns = ['id', 'sen'] + get_sen_labels() + ['len', 'pos', 'tf', 'df', 'rouge_2']\n",
    "valid_df.len = len_scaler.transform(valid_df.len.values.reshape(-1, 1))\n",
    "print(\"Shape:\", valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_pickle('data/valid_set.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試集\n",
    "### 與驗證集做一樣的處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list, sen_list, sentences_list, pos_list = [], [], [], []\n",
    "for _, row in test_df.iterrows():\n",
    "    id, document = row.id, row.document\n",
    "    doc_len = len([word for sen in document for word in sen])\n",
    "    \n",
    "    word_cursor = 0\n",
    "    for sentence in document:\n",
    "        word_cursor += len(sentence)\n",
    "        pos_list.append(word_cursor / doc_len)\n",
    "    \n",
    "    document = list(map(padding, document))\n",
    "    for _ in range(M):\n",
    "        document.insert(0, [])\n",
    "    for _ in range(N):\n",
    "        document.append([])\n",
    "    \n",
    "    for i in range(len(document)-M-N):\n",
    "        id_list.append(row.id)\n",
    "        sen_list.append(i)\n",
    "        sentences_list.append(document[i:i+M+N+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = pd.DataFrame(id_list)\n",
    "sen_df = pd.DataFrame(sen_list)\n",
    "sentences_df = pd.DataFrame(sentences_list)\n",
    "pos_df = pd.DataFrame(pos_list)\n",
    "\n",
    "sentence_df = pd.concat([id_df, sen_df, sentences_df], axis=1)\n",
    "sentence_df.columns = ['id', 'sen'] + get_sen_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e8d6a85404405693f229eb0c984c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='len', max=156228), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffb7d04077c49c4a8faa7141321dc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='tf', max=156228), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99744c9e90034221bbe3bcf7e7fa610b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='df', max=156228), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1a6caea09746dd9cc7725be6b12341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='ROUGE-2', max=156228), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm_notebook.pandas(desc=\"len\")\n",
    "len_df = sentence_df.st.progress_apply(lambda x: len(x)-2)\n",
    "tqdm_notebook.pandas(desc=\"tf\")\n",
    "tf_df = sentence_df.progress_apply(get_tf, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"df\")\n",
    "df_df = sentence_df.progress_apply(get_df, axis=1)\n",
    "tqdm_notebook.pandas(desc=\"ROUGE-2\")\n",
    "rouge_2_df = sentence_df.progress_apply(get_rouge_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (156228, 18)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.concat([sentence_df, len_df, pos_df, tf_df, df_df, rouge_2_df], axis=1)\n",
    "test_df.columns = ['id', 'sen'] + get_sen_labels() + ['len', 'pos', 'tf', 'df', 'rouge_2']\n",
    "test_df.len = len_scaler.transform(test_df.len.values.reshape(-1, 1))\n",
    "print(\"Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle('data/test_set.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
